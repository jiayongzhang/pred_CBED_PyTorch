# -*- coding: utf-8 -*-
"""pred_CBED.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1so8i-5nk4xpYEuXOXbHDgkIfe8b1LLav

#TODO
1. ~~~Normalize data power(0.25), and between -1 and 1:
    https://github.com/fab-jul/hdf5_dataloader~~~
2. Use prefetch to accelerate data access: https://zhuanlan.zhihu.com/p/66145913
3. Use transfer learning from Imagenet: https://medium.com/@josh_2774/deep-learning-with-pytorch-9574e74d17ad?
4. ~~~Add EDA for space imbalance~~~(had been separated as count.py)
5. Add save and restart function
6. ~~~Check if it's the right way to permute matrix~~~, and tune parameters
7. ~~~Add dropout layers~~~
8. Add parallel acceleration: distributedparallelcpu, horovod https://github.com/horovod/tutorials/tree/master/fashion_mnist
https://github.com/vlimant/mpi_learn
https://blog.csdn.net/zwqjoy/article/details/89415933
9. Imbalanced data
https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/
10. ~~~Add support for multiple pred~~~
11. can we really train on different hf files, instead on a total run?
"""

from __future__ import print_function, division
import os
import time
import h5py
#import shutil
#import collections
import glob
#import pylab
import pandas as pd
import numpy as np
#from tqdm import tqdm, tqdm_notebook as tn
from PIL import Image
#import matplotlib.pyplot as plt
#from skimage import io, transform
from prefetch_generator import BackgroundGenerator, background
import torch
from torchvision import transforms, datasets, utils
import torch.nn as nn
#import torch.nn.functional as F
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim

# defined functions
from tools import my_transforms
from tools.my_model import Model
from tools.my_prefetcher import data_prefetcher

# hdf5 dataloader
from hdf5_dataloader.src.dataset import HDF5Dataset
from hdf5_dataloader.src.transforms import ArrayToTensor, ArrayCenterCrop
 
# Ignore warnings
import warnings
warnings.filterwarnings("ignore")
 
# %matplotlib inline

def get_weights(filename):
    count_data = pd.read_csv(filename)
    weights = 1.0/count_data['weight']
    weights[weights==np.inf] = 1000.0

    return weights

# define initial variables
def init():
    global device, weights, train_paths, dev_paths
    global TRAIN_BATCH_SIZE, DEV_BATCH_SIZE, OMP_NUM_THREADS

    OMP_NUM_THREADS = os.environ["OMP_NUM_THREADS"]

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(device)

    weights = get_weights('count.csv')
    train_paths = glob.glob('./train/' + '*.h5')
    dev_paths = glob.glob('./dev/' + '*.h5')

    TRAIN_BATCH_SIZE = 32
    DEV_BATCH_SIZE = 1
    

class CBEDDataset(Dataset):
    """ CBED dataset."""

    def __init__(self, data_file, transform=None):
        self.transform = transform
        self.x_data = None
        self.y_data = None
        
        self.__read_hdf5__(data_file)
        
    def __read_hdf5__(self, data_file):
        f = h5py.File(data_file, mode='r', swmr=True)
        tmp_x, tmp_y = [], []
        for key in f.keys():
            tmp_x.append(f[key]['cbed_stack'][()])
            # indexed from 0 to avoid overflow error
            tmp_y.append(int(f[key].attrs['space_group'])-1)
        f.close()
        #tmp_y = map(int, tmp_y)
        #self.x_data = torch.from_numpy(np.array(tmp_x))
        #self.y_data = torch.from_numpy(np.array(tmp_y))
        self.x_data = torch.Tensor(tmp_x)
        self.y_data = torch.Tensor(tmp_y).long()
        
    def __getitem__(self, index):
        #_x = self.x_data[index]
        if self.transform is not None:
            self.x_data[index] = self.transform(self.x_data[index])
            #_x = self.transform(_x)
        return self.x_data[index], self.y_data[index]

    def __len__(self):
        return len(self.y_data)

transform_hdf5 = transforms.Compose([ArrayCenterCrop(256),
                                     ArrayToTensor()])

data_transform = transforms.Compose([
    my_transforms.TensorPower(0.25),
    transforms.ToPILImage(),
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.ToTensor(),
    #transforms.Normalize(mean = [0.5,0.5,0.5],std = [0.5,0.5,0.5])
])

def train(epoch=0):
    model.train()
    for data, target in BackgroundGenerator(train_loader, max_prefetch = 3):
    #prefetcher = data_prefetcher(train_loader)
    #data, target = prefetcher.next()
    #while data is not None:
        data, target = Variable(data).to(device), Variable(target).to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        #data, target = prefetcher.next()
        
    #end = time.time()
    #print('Training: Timing: {:.2f} s\tLoss: {:.6f}'.format(
    #      end-begin, loss.item())

    return loss.item()

def dev():
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in BackgroundGenerator(dev_loader, max_prefetch = 3):
        data = Variable(data, volatile=True).to(device)
        target = Variable(target).to(device)
        output = model(data)
        # sum up batch loss
        test_loss += criterion(output, target).cpu().data.numpy()
        # get the index of the max
        pred = output.data.max(1, keepdim=True)[1][0]
        print(pred+1, target+1)
        correct += pred.eq(target.data.view_as(pred)).sum()

    test_loss /= len(dev_loader.dataset)
    print('\nTest set:\n')
    print('Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(dev_loader.dataset),
        100. * correct / len(dev_loader.dataset)))
    
# multiple predictions
def dev_multipred():
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in BackgroundGenerator(dev_loader, max_prefetch = 3):
        data = Variable(data, volatile=True).to(device)
        target = Variable(target).to(device)
        output = model(data)
        # sum up batch loss
        test_loss += criterion(output, target).cpu().data.numpy()
        # get the index of the max
        #pred = output.data.max(1, keepdim=True)[1][0]
        sorted, preds = torch.sort(output.data[0], descending=True)
        #print(indices[0:5]+1, target+1)
        #correct += pred.eq(target.data.view_as(pred)).sum()
        if target in preds[:5]:
            correct += 1

    test_loss /= len(dev_loader.dataset)
    print('\nTest set:\n')
    print('Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(dev_loader.dataset),
        100. * correct / len(dev_loader.dataset)))

#================================================================
# Run starts from here
#================================================================

init()
model = Model()
#if torch.cuda.device_count() > 1:
#    print("We are running on {} GPUs!\n".format(torch.cuda.device_count()))
#    model = nn.DataParallel(model)
model.to(device)

# use softmax
#weights = np.exp(count_data['weight'])
class_weights = torch.Tensor(weights).to(device)

criterion = nn.CrossEntropyLoss(weight=class_weights)
#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.99))
#if torch.cuda.device_count() > 1:
#    optimizer = nn.DataParallel(optimizer)

# prefetch multiple files
# TODO: send multiple h5 files indices to CBEDDataset, and read it at once
# Then prefetch this
for this_file in (train_paths):
    #print("Current file: %s"%this_file)
    read_start = time.time()
    try:
        train_dataset = CBEDDataset(this_file,
                                    transform = data_transform)
    except:
        print("Warning: error handling %s, will be ignored."%this_file)
        continue
    else:
        train_loader = DataLoader(dataset=train_dataset,
                                  batch_size=TRAIN_BATCH_SIZE,
                                  shuffle=True,
                                  pin_memory=True,
                                  num_workers=OMP_NUM_THREADS)
    
    read_end = time.time()
    print('Reading {} tooks {} s\n'.format(this_file, read_end-read_start))
    begin = time.time()
    for i in range(10):
        loss = train()
        print(loss),
    end = time.time()
    print('Training batch:\tTiming: {:.2f} s,\tLoss: {:.6f}'.format(
          end-begin, loss))

# test
for this_file in (dev_paths):
    #print("Current file: %s"%this_file)
    try:
        dev_dataset = CBEDDataset(this_file,
                                  transform = data_transform)
    except:
        print("Warning: error handling %s, will be ignored."%this_file)
        continue
    else:
        dev_loader = DataLoader(dataset=dev_dataset,
                                batch_size=DEV_BATCH_SIZE,
                                shuffle=False,
                                pin_memory=True,
                                num_workers=OMP_NUM_THREADS)
    
    dev_multipred()
